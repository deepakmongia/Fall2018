---
title: "Data 607 Final Project - Part-2 : Tweets Cleaning and Analysis"
author: "Deepak Mongia"
date: "December 8, 2018"
output: html_document
---
###Tweets Clean-up and analysis
```{r setup, include=FALSE}
library(rtweet)
library(RCurl)
library(jsonlite)
library(dplyr)
library(tm)
library(wordcloud)
library(wordcloud2)
library(SentimentAnalysis)
library(RMySQL)
library(stringr)
library(kableExtra)
```

This is the second part of the project, where we will get the data from the data lake and clean it, store into MySQL, and perform the data analysis on the tweets text.

The below part if just a creation of the tweet data thru the API call, and will be replaced with MongoDB data fetch.
```{r cachedChunk, cache=TRUE}
create_token("607 Final Project",
            consumer_key = "xxx",
             consumer_secret = "xxx",
             access_token = "xxx",
             access_secret = "xxx")

iphone_xs_raw_tweets <- search_tweets("iphone xs", n = 18000, include_rts = FALSE, retryonratelimit = TRUE, lang = "en")

dim(iphone_xs_raw_tweets)

##iphone_xs_raw_tweets[1:10] %>% kable() %>% kable_styling()
```

```{r cachedChunk2, cache=TRUE}
iphone_xs_raw_tweets %>% head(4) %>% kable() %>% kable_styling()

### Get only the required fields
iphone_xs_tweets_required <- iphone_xs_raw_tweets %>% select(1:5, is_retweet, lang, status_url, location)

##Now, grouping the tweets base don the screen_name
iphone_xs_tweets_required_group_user <- iphone_xs_tweets_required %>% group_by(screen_name)
per_user <- summarise(iphone_xs_tweets_required_group_user, number_of_messages=n())

# Arranging the tweets count in the descending order, so that the user (screen_name) with the highest count is on the top.
per_user <- arrange(per_user, desc(number_of_messages))

head(per_user)

```

## Now as we see above, there are close to 300 users on the top which have a bigger numer of tweets on the same topic - iphone xs. So, to lessen the occurences or impact of the advertizements in our analysis, we are removing the users from our analysis who have 5 or more tweets on this topic.

```{r cachedChunk3, cache=TRUE}
per_user <- per_user %>% filter(number_of_messages < 5)

## Joining the 2 data.frames, and getting the messages only for the users with screen names having 4 or fewer messages count
iphone_xs_tweets_required_users <- merge(iphone_xs_tweets_required, per_user, by="screen_name")

dim(iphone_xs_tweets_required_users)

##using only the text from the data.frame. The remaining fields will not be required, for further steps. Hence droping them.
iphone_xs_tweets_required_text <- iphone_xs_tweets_required_users %>% select(5)

iphone_xs_tweets_required_text %>% head(4) %>% kable() %>% kable_styling()
```

## Storing the text of all the required tweets in a MySQL database. This will help easy fetch from the database whenever we need it for our analysis.
Initial few code lines are to set up the MySQL schema and table structure so the tweet text can be written to MySQL table.
##1. Connecting to the database, and creating schema and table.
```{r}
## Connecting to MySQL
mydb = dbConnect(MySQL(), user='root', password='Deepak1234#', dbname='twitter', host='localhost')
summary(mydb)

## Drop the twitter schema if it already exists on the MySQL database
#drop.schema.query <- paste("drop schema if exists `twitter`;")
#dbGetQuery(mydb, drop.schema.query)

## Create the twitter schema
#create.schema.query <- paste("CREATE SCHEMA `twitter`;")
#dbGetQuery(mydb, create.schema.query)

## Creating the new table 'twitter' for storig the tweets for iphone_xs
#drop.MySQL.table.query <- paste("DROP TABLE IF EXISTS `twitter`;")
#dbGetQuery(mydb, drop.MySQL.table.query)

## Note here, that even if the twitter tweet has a character limit of 140 characters, but there are some tweets which have refernces to multiple users making the actual character count bigger. The user names refered using @ are not counted in the tweet character length count. Hence, to be on a safer side, we have created the tweet_text of length 1000.
## If we created a shorter length of 500, we got an error for a few tweets which actually became more than 500 characters due to references to multiple users.
iphone_xs_tweets_required_text[10807,1]
nchar(iphone_xs_tweets_required_text[10807,1])

#create.MySQL.table.query <- paste("CREATE TABLE `twitter` (
#  `tweet_text` varchar(1000) NOT NULL
#);")

```

## Storing the data (tweet text) into MySQL table.
```{r cachedChunk5, cache=TRUE}
## There were some tweets with double quote, replacing it with a space to avoid any error while interting into MySQL
iphone_xs_tweets_required_text$text <- str_replace_all(iphone_xs_tweets_required_text$text, '"', ' ')

no_of_tweets <- nrow(iphone_xs_tweets_required_text)

query <- paste("INSERT INTO twitter VALUES (", '"', iphone_xs_tweets_required_text[1,], '")', collapse = '')
query
#dbGetQuery(mydb, query)


## Running a loop to load the whole data frame (All tweets) into MySQL table
for (i in 1:no_of_tweets) {
  query <- paste("INSERT INTO twitter VALUES (", '"', iphone_xs_tweets_required_text[i,], '")', collapse = '')
  dbGetQuery(mydb, query)
}

```

## Now get the tweets from the MySQL data base
```{r cachedChunk6, cache=TRUE}
get.MySQL.table.query <- paste("select * from twitter;")
iphone_xs_tweets_from_MySQL_df <- dbGetQuery(mydb, get.MySQL.table.query)

```

## Text Mining
```{r}
### Loading the tweets text into a corpus
myCorpus <- Corpus(VectorSource(iphone_xs_tweets_from_MySQL_df$tweet_text))
inspect(myCorpus[1:20])

### Converting to lower characters
myCorpus <- tm_map(myCorpus, content_transformer(tolower))

## Remove http
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))

## Remove any junk characters
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))

## Remove stopwords
myStopwords <- c(setdiff(stopwords('english'), c("r", "big")),
                 "use", "see", "used", "via", "amp")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)

## Strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)

## Stemming
myCorpus <- tm_map(myCorpus, stemDocument) # stem words

inspect(myCorpus[1:20])


## Creating Term Document Matrix
tdm <- TermDocumentMatrix(myCorpus)

tdm <- as.matrix(tdm)
tdm[1:10, 1:20]

## As we see here, iphon is the word in every document, which is justified, as we searched for iphone xs. Hence we will remove this term - iphon, as it does not solve any purpose here, as it will be present in all the tweets fetched.
myCorpus <- tm_map(myCorpus, removeWords, c("iphon", "xs"))

## Building the tdm again after removing iphon word:
tdm <- TermDocumentMatrix(myCorpus)

tdm <- as.matrix(tdm)
tdm[1:10, 1:20]


## Building the word frequency
word.freq <- rowSums(tdm)
word.freq <- sort(word.freq, decreasing = TRUE)
set.seed(222)

## Top 20 used words
word.freq[1:20]


## Building the word cloud
twitter.words <- names(word.freq)

wordcloud(words = twitter.words, 
          freq = word.freq,
          max.words = 150,
          random.order = F,
          min.freq = 5,
          colors = brewer.pal(8, 'Dark2'),
          scale = c(5,0.3),
          rot.per = 0.3)

```
